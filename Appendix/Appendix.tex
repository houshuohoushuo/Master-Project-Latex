\chapter{Program Manual} % Main appendix title
\label{appendixA}

This program is implemented in Python 3.6.
Required packages: keras, sklearn, pandas, numpy, PIL, pyyaml.

\textbf{split\_stack.py}: The original PAT and US images are in a three-dimensional image stack(.tif) format. This script is to quickly process these stacks and generate corresponding sets of 2D images.
The original dataset has the following directory structure:

\dirtree{%
.1 dataset.
.2 AF014.
.3 PAT 930.
.4 014 PAT 930 initial stack.tif .
.3 US.
.4 14.tif .
.2 AF018.
.2 ..
.2 ..
}
The output directory structure produced by the script is
\dirtree{%
.1 PAT\_extracted.
.2 AF014.
.3 014\_15.jpg .
.3 014\_16.jpg .
.3 014\_17.jpg .
.3 014\_18.jpg .
.3 014\_19.jpg .
.3 014\_20.jpg .
.2 AF018.
.2 ..
.2 ..
}

\textbf{data.py}: split\_data\_folder partitions the data into a training set and a validation set according to a K-fold specification. This directory structure is specifically required by flow\_from\_directory() for training (this step will be done automatically in training). The directory structure is:
\dirtree{%
.1 data .
.2 train.
.3 B .
.4 014\_15.jpg .
.4 014\_16.jpg .
.4 ..
.3 C .
.4 027\_14.jpg .
.4 027\_15.jpg .
.4 ..
.2 valid.
.3 B .
.4 ..
.3 C .
.4 ..
}

\textbf{train.py}: This is the main function. It reads the training data, performs K-fold partitioning, trains the model, and reports training and validation loss and accuracy. A configuration YAML file (train.yaml) should be provided to train.py. The path to the dataset, model used, and several training parameters are specified in this file.

The training parameters can be modified in \textbf{train.yaml}.

To run the model, execute first:

\textbf{python split\_stack.py train}

and then execute:

\textbf{python train.py train}

Tuning training parameters could be tedious. \textbf{generate\_models.py} is to automatically generate and run models with different training parameters specified in \textbf{models.yaml}. This program generates models with all combinations of parameters, including model name, number of epoch, batch size, etc., and runs these models locally or batches them for execution on SHARCNET.